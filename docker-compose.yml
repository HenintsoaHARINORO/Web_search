version: '3.8'

services:
  # Ollama LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      - ollama
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        sleep 5
        echo "Pulling llama3.2:3b model..."
        ollama pull llama3.2:3b
        echo "Model ready!"
    environment:
      - OLLAMA_HOST=ollama:11434
    restart: "no"

  # Portfolio Streamlit App
  portfolio-app:
    build: .
    container_name: portfolio-assistant
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./portfolio_vectorstore:/app/portfolio_vectorstore
    env_file:
      - .env
    environment:
      - OLLAMA_API_URL=http://ollama:11434/api/chat
      - OLLAMA_API=http://ollama:11434
    depends_on:
      - ollama
      - ollama-init
    restart: unless-stopped

volumes:
  ollama_data: