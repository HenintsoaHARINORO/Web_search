import csv, json
import os
from typing import List, Dict
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_classic.schema import Document
from langchain_ollama import OllamaLLM
from langchain_classic.chains import RetrievalQA
from langchain_classic.prompts import PromptTemplate
from config import VECTOR_STORE_PATH, OLLAMA_MODEL, OLLAMA_API, PORTFOLIO_FILE


class PortfolioRAG:
    # RAG pour chercher et naviguer dans les portfolios
    def __init__(self, csv_file: str = PORTFOLIO_FILE, vector_store_path: str = VECTOR_STORE_PATH):
        self.csv_file = csv_file
        self.vector_store_path = vector_store_path
        self.metadata_file = os.path.join(vector_store_path, "index_metadata.json")
        self.embeddings = OllamaEmbeddings(model=OLLAMA_MODEL, base_url=OLLAMA_API)
        self.llm = OllamaLLM(model=OLLAMA_MODEL, base_url=OLLAMA_API, temperature=0.3)
        self.vectorstore = None
        self.qa_chain = None

    def _get_csv_modification_time(self) -> float:
        """Get the last modification time of the CSV file."""
        if os.path.exists(self.csv_file):
            return os.path.getmtime(self.csv_file)
        return 0

    def _load_index_metadata(self) -> dict:
        """Load metadata about the last indexation."""
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {"last_csv_mtime": 0, "indexed_companies": []}

    def _update_vectorstore_incrementally(self):
        """Identify new documents and merge them with the existing index."""
        metadata = self._load_index_metadata()
        existing_company_names = set(metadata.get("indexed_companies", []))

        # Also check from vectorstore if metadata is empty
        if not existing_company_names and self.vectorstore:
            for doc_id in self.vectorstore.docstore._dict:
                doc = self.vectorstore.docstore.lookup(doc_id)
                if doc and doc.metadata.get("company_name"):
                    existing_company_names.add(doc.metadata["company_name"])

        all_documents = self.load_portfolio_data()
        new_documents = [
            doc for doc in all_documents
            if doc.metadata["company_name"] not in existing_company_names
        ]

        if not new_documents:
            print("Aucune nouvelle donn√©e √† indexer. Le vector store est √† jour.")
            return

        print(f"Indexation de {len(new_documents)} nouveau(x) document(s)...")
        new_faiss_index = FAISS.from_documents(new_documents, self.embeddings)
        self.vectorstore.merge_from(new_faiss_index)
        self.vectorstore.save_local(self.vector_store_path)

        # Update metadata with all companies
        all_company_names = [doc.metadata["company_name"] for doc in all_documents]
        self._save_index_metadata(all_company_names)
        print(f"Fusion r√©ussie : {len(new_documents)} documents ajout√©s.")

    def load_portfolio_data(self) -> List[Document]:
        documents = []

        if not os.path.exists(self.csv_file):
            print(f"‚ùå Fichier {self.csv_file} non trouv√©")
            return documents

        with open(self.csv_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            for row in reader:
                # Cr√©er un contenu structur√© pour chaque entreprise
                content = f"""
Entreprise: {row['company_name']}

R√©sum√©:
{row['resume']}

Commentaires:
{row['comments'] if row['comments'] else 'Aucun commentaire'}
"""

                doc = Document(
                    page_content=content,
                    metadata={
                        "company_name": row['company_name'],
                        "source": "portfolio_csv"
                    }
                )
                documents.append(doc)

        print(f"{len(documents)} entreprise(s) charg√©e(s)")
        return documents

    def _save_index_metadata(self, indexed_companies: List[str]):
        """Save metadata about the current indexation."""
        os.makedirs(os.path.dirname(self.metadata_file), exist_ok=True)
        metadata = {
            "last_csv_mtime": self._get_csv_modification_time(),
            "indexed_companies": indexed_companies
        }
        with open(self.metadata_file, 'w') as f:
            json.dump(metadata, f)

    def _needs_update(self) -> bool:
        """Check if the CSV file was modified since last indexation."""
        metadata = self._load_index_metadata()
        current_mtime = self._get_csv_modification_time()
        return current_mtime > metadata["last_csv_mtime"]

    def build_vectorstore(self, force_rebuild: bool = False):
        """Build or load the FAISS vector store with automatic update detection."""
        if not os.path.exists(self.csv_file):
            print(f"‚ùå Fichier source CSV {self.csv_file} non trouv√©.")
            return

        vectorstore_exists = os.path.exists(self.vector_store_path)
        needs_update = self._needs_update()

        if vectorstore_exists and not force_rebuild:
            print("Chargement du vector store existant...")
            try:
                self.vectorstore = FAISS.load_local(
                    self.vector_store_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("Vector store charg√©.")

                # Check if CSV was modified -> incremental update
                if needs_update:
                    print("üìù Modifications d√©tect√©es dans le CSV...")
                    self._update_vectorstore_incrementally()
                else:
                    print("Index d√©j√† √† jour.")
                return
            except Exception as e:
                print(f"Erreur lors du chargement : {e}. Reconstruction forc√©e...")
                force_rebuild = True

        if not vectorstore_exists or force_rebuild:
            print("Construction compl√®te du vector store...")
            documents = self.load_portfolio_data()
            if not documents:
                print("Aucune donn√©e √† indexer")
                return

            self.vectorstore = FAISS.from_documents(documents, self.embeddings)
            self.vectorstore.save_local(self.vector_store_path)

            # Save metadata
            company_names = [doc.metadata["company_name"] for doc in documents]
            self._save_index_metadata(company_names)
            print(f"Vector store cr√©√© et sauvegard√© dans {self.vector_store_path}")

    def rebuild_index(self):
        """Force la reconstruction de l'index (√† appeler apr√®s mise √† jour du CSV)"""
        print("Reconstruction de l'index...")
        self.build_vectorstore(force_rebuild=True)

    def setup_qa_chain(self):
        """Configure la cha√Æne de questions-r√©ponses"""
        if self.vectorstore is None:
            print("Vector store non initialis√©")
            return
        num_docs = len(self.vectorstore.docstore._dict)
        k_value = max(num_docs, 2)
        # Template de prompt personnalis√©
        template = """Tu es un assistant sp√©cialis√© dans l'analyse de portefeuille d'entreprises.
Utilise les informations suivantes pour r√©pondre √† la question de mani√®re pr√©cise et professionnelle.

Contexte:
{context}

Question: {question}

Instructions:
- Si la question concerne une entreprise sp√©cifique, donne tous les d√©tails pertinents
- R√©ponds toujours en fran√ßais

R√©ponse:"""

        PROMPT = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": k_value}  # nombre de documents √† retourner
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )

        print("Cha√Æne QA configur√©e")

    def search(self, query: str, k: int = 3) -> List[Document]:
        """Recherche par similarit√© dans le vector store"""
        if self.vectorstore is None:
            print("Vector store non initialis√©")
            return []

        results = self.vectorstore.similarity_search(query, k=k)
        return results

    def ask(self, question: str) -> Dict:
        """Pose une question sur le portefeuille"""
        if self.qa_chain is None:
            print("Cha√Æne QA non initialis√©e")
            return {"answer": "Syst√®me non initialis√©", "sources": []}

        result = self.qa_chain.invoke({"query": question})

        return {
            "answer": result["result"],
            "sources": result["source_documents"]
        }

    def list_all_companies(self) -> List[str]:
        """Liste toutes les entreprises du portefeuille"""
        documents = self.load_portfolio_data()
        return [doc.metadata["company_name"] for doc in documents]

    def find_companies_by_keyword(self, keyword: str) -> List[Document]:
        """Trouve les entreprises contenant un mot-cl√©"""
        return self.search(keyword, k=5)


def initialize_rag(rebuild: bool = False) -> PortfolioRAG:
    """Initialise le syst√®me RAG"""
    rag = PortfolioRAG()
    rag.build_vectorstore(force_rebuild=rebuild)
    rag.setup_qa_chain()
    return rag
